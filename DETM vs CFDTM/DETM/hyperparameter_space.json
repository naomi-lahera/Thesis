{
    "num_topics": [20, 50, 100],
    "rho_size": [100, 200, 300],
    "en_units": [400, 800, 1200],
    "eta_hidden_size": [100, 200, 300],
    "enc_drop": [0.0, 0.2, 0.3, 0.5],
    "eta_nlayers": [1, 2, 3],
    "eta_dropout": [0.0, 0.2, 0.3, 0.5],
    "delta": [0.001, 0.005, 0.01, 0.05],
    "theta_act": ["relu", "tanh", "softplus", "leakyrelu"]
}