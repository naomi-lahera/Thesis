{
    "num_topics": [20, 50, 100],
    "rho_size": [200, 300, 400],
    "en_units": [400, 800, 1200],
    "eta_nlayers": [2, 3, 4],
    "delta": [0.001, 0.005, 0.01, 0.05],
    "theta_act": ["anh", "softplus", "relu", "rrelu", "leakyrelu", "elu", "selu", "glu"],
    "eta_hidden_size": [100, 200, 300],


    "enc_drop": [0.0, 0.2, 0.3, 0.5],
    "eta_dropout": [0.0, 0.2, 0.3, 0.5],
    "learning_rate": [1e-3, 5e-4, 0.02],
    "epochs": [200, 400, 800]
}